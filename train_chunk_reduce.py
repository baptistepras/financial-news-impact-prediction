"""
Train and run a finance-oriented “impact summarizer” for financial news articles.

The script reads a CSV dataset containing financial news articles and their reference impact summaries
generated by Mixtral. Each article is split into overlapping token chunks. A notes model first extracts 
factual bullet points per chunk, and a second model produces a concise document-level impact summary 
by reducing these notes (recursively, to avoid context overflow). During training, the impact model is 
fine-tuned on chunked inputs with a higher loss weight on the first chunk to emphasize front-loaded 
financial information. The final output is a CSV with model predictions and ROUGE-L scores.
"""

import argparse
import math
import os
import re
from typing import Dict, List, Tuple

import fasttext
import numpy as np
import pandas as pd
import torch
from datasets import Dataset
from rouge_score import rouge_scorer
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    set_seed,
)

# Role prompt to guide the model's tone and focus on financial analysis without opinions or advice
ANALYST_ROLE = (
    "You are a senior sell-side financial analyst writing for institutional investors.\n"
    "Summarize disclosures with focus on business and shareholder impact.\n"
)

# Prompt for extracting factual bullet points from each chunk, emphasizing strict adherence to facts and financial details
NOTES_PROMPT = (
    ANALYST_ROLE
    + "Task: Extract 4–8 factual bullet points from the passage.\n"
    "Main Subject (issuer): {main_subject}\n"
    "Rules (STRICT):\n"
    "- Bullets must be factual and specific (WHO did WHAT, incl. counterparties).\n"
    "- Preserve ALL numbers, units, currencies, durations, and percentages exactly as written.\n"
    "- Preserve technical financial/accounting terms verbatim (e.g., 'Net Income', 'EBITDA').\n"
    "- If multiple entities are mentioned, attribute each action to the correct entity explicitly.\n"
    "- NO opinions, NO investor advice, NO generic statements.\n"
    "- Output ONLY bullet points starting with '-'.\n\n"
    "PASSAGE:\n"
)

# Few-shot examples to demonstrate the expected structure, factual density, and style of the impact summaries, 
# with a focus on financial events and their implications for fundamentals and shareholders
IMPACT_FEWSHOT = (
    "EXAMPLE 1\n"
    "Main Subject (issuer): Alpha Corp\n"
    "NOTES:\n"
    "- Alpha Corp signed a 3-year supply agreement valued at $120 million.\n"
    "- The contract starts in Q3 2024 and covers 50,000 units per year.\n"
    "- Counterparty: Beta Industries.\n"
    "IMPACT SUMMARY:\n"
    "The Alpha Corp signed a 3-year, $120 million supply agreement with Beta Industries starting in Q3 2024. "
    "This increases revenue visibility and may support capacity planning given the 50,000 units/year commitment.\n\n"
    "EXAMPLE 2\n"
    "Main Subject (issuer): Delta plc\n"
    "NOTES:\n"
    "- Delta plc reported Q1 revenue of €480 million (up 6% YoY) and Net Income of €52 million.\n"
    "- Management reiterated FY guidance and announced a €100 million share buyback.\n"
    "IMPACT SUMMARY:\n"
    "The Delta plc reported Q1 revenue of €480 million (+6% YoY) and Net Income of €52 million while reiterating FY guidance. "
    "The €100 million share buyback could increase shareholder returns and signal balance-sheet flexibility.\n\n"
)

# Prompt for the final impact summary generation, emphasizing strict adherence to the extracted 
# notes and a structured, factual summary format that starts with the main subject and focuses 
# on the primary event and its implications for fundamentals and shareholders
IMPACT_PROMPT = (
    ANALYST_ROLE
    + "Task: Write an IMPACT SUMMARY in 2–4 sentences.\n"
    "Main Subject (issuer): {main_subject}\n\n"
    "Hard constraints (STRICT):\n"
    "- Use ONLY facts present in the NOTES.\n"
    "- Do NOT introduce new entities (companies/people) not in NOTES.\n"
    "- Preserve ALL numbers/units/currencies/durations exactly if present in NOTES.\n"
    "- Preserve technical financial/accounting terms verbatim.\n"
    "- NO investor advice, NO hype, NO clichés, NO vague filler.\n"
    "- Prefer short, information-dense sentences; avoid repetition.\n\n"
    "Required structure:\n"
    "1) Sentence 1 MUST start with exactly: \"The {main_subject} ...\" and state the primary event/action.\n"
    "2) Sentence 2 explains why it matters (fundamentals/shareholders) using facts from NOTES.\n"
    "3) Optional sentence 3–4 only if it adds concrete risk/financial detail.\n\n"
    + IMPACT_FEWSHOT
)

IMPACT_TRAIN_PREFIX = IMPACT_PROMPT.format(main_subject="{main_subject}") + "\nTEXT:\n"

_WHITESPACE_RE = re.compile(r"\s+")
_MULTI_NL_RE = re.compile(r"\n{2,}")

# Common boilerplate headings in financial disclosures that should be stripped 
# from chunk inputs and model outputs to reduce noise and prevent the model 
# from learning to copy them instead of generating meaningful content
_HEADING_LINE_RE = re.compile(
    r"^\s*("
    r"(table\s+of\s+contents)|"
    r"(forward[-\s]*looking\s+statements?)|"
    r"(cautionary\s+statement)|"
    r"(safe\s+harbor)|"
    r"(introduction)|"
    r"(element\s+list\s+explanation)|"
    r"(exhibit\s+\d+(\.\d+)?)|"
    r"(item\s+\d+(\.\d+)?)|"
    r"(signature(s)?)|"
    r"(index\s+to\s+exhibits)|"
    r"(risk\s+factors?)|"
    r"(about\s+the\s+company)|"
    r"(contacts?)|"
    r"(press\s+release)|"
    r"(legal\s+disclaimer)|"
    r"(definitions?)"
    r")\s*[:\-]?\s*$",
    flags=re.IGNORECASE,
)

# Inline headings that often appear in the middle of chunks or model outputs, 
# which should be removed to reduce noise and prevent the model from copying 
# them instead of generating meaningful content
_INLINE_HEADING_RE = re.compile(
    r"(element\s+list\s+explanation|table\s+of\s+contents|forward[-\s]*looking\s+statements?|safe\s+harbor|"
    r"introduction|press\s+release)\s*[:\-]\s*",
    flags=re.IGNORECASE,
)

# Cliché phrases commonly found in financial news summaries that add little
#  informational value and should be removed when possible, while ensuring
#  that enough factual content remains for a meaningful summary
_CLICHE_RE = re.compile(
    r"(investors?\s+(should|may)|stay\s+informed|significant\s+milestone|major\s+milestone|game\s+changer|"
    r"key\s+step\s+forward|ultimately\s+benefiting|positive\s+sign|potentially\s+leading\s+to|"
    r"increased\s+interest|global\s+economy|positions?\s+the\s+company|underscores\s+its)",
    flags=re.IGNORECASE,
)

_SENT_SPLIT_RE = re.compile(r"(?<=[.!?])\s+")

# Heuristic patterns to extract the main subject (issuer/company name) from the start of financial disclosures,
# which often follow certain formats such as "Company Name (NYSE: TICKER)" or "Company Name (the 'Company')", 
# or are mentioned in the first few sentences with key verbs like "announced" or "reported"
_ISSUER_PATTERNS = [
    re.compile(
        r"^\s*([A-Z][A-Za-z0-9&.,'’\-\s]{2,80}?)\s+\((?:NYSE|NASDAQ|AMEX|OTCQX|OTC|LSE|TSX|ASX|HKEX|FWB|SWX)[:\s][A-Z.\-]{1,12}\)",
        re.MULTILINE,
    ),
    re.compile(r"\b([A-Z][A-Za-z0-9&.,'’\-\s]{2,80}?)\s+\((?:the\s+)?\"Company\"\)", re.IGNORECASE),
    re.compile(r"\b([A-Z][A-Za-z0-9&.,'’\-\s]{2,80}?)\s+announced\b", re.IGNORECASE),
    re.compile(r"\b([A-Z][A-Za-z0-9&.,'’\-\s]{2,80}?)\s+reported\b", re.IGNORECASE),
    re.compile(r"\b([A-Z][A-Za-z0-9&.,'’\-\s]{2,80}?)\s+today\b", re.IGNORECASE),
]


def make_chunk_notes_prompt(chunk_text: str, main_subject: str) -> str:
    """Build the prompt used to extract factual bullet notes from a single chunk."""
    return NOTES_PROMPT.format(main_subject=main_subject) + (chunk_text or "").strip() + "\n\nBULLETS:\n"


def make_reduce_prompt(chunk_notes: List[str], main_subject: str) -> str:
    """Build the prompt used to produce the final impact summary from a list of chunk-level notes."""
    notes = "\n".join([ln.strip() for ln in chunk_notes if isinstance(ln, str) and ln.strip()])
    return (
        IMPACT_PROMPT.format(main_subject=main_subject)
        + "\nNOTES (use ONLY these facts):\n"
        + notes
        + "\n\nIMPACT SUMMARY:\n"
    )


def clean_text(x: str) -> str:
    """Normalize whitespace and line breaks while preserving the original content."""
    if not isinstance(x, str):
        return ""
    x = x.replace("\u00a0", " ").replace("\r", "\n")
    x = _MULTI_NL_RE.sub("\n", x)
    return x.strip()


def strip_boilerplate_lines(text: str) -> str:
    """Remove common boilerplate headings and normalize whitespace for model input."""
    t = clean_text(text)
    lines: List[str] = []
    for ln in t.split("\n"):
        l = ln.strip()
        if not l:
            continue
        if _HEADING_LINE_RE.match(l):
            continue
        lines.append(l)
    t = "\n".join(lines)
    t = _INLINE_HEADING_RE.sub("", t)
    t = t.replace("\n", " ")
    t = _WHITESPACE_RE.sub(" ", t).strip()
    return t


def _remove_cliches_safely(text: str) -> str:
    """Remove cliché sentences when enough information remains."""
    sents = [s.strip() for s in _SENT_SPLIT_RE.split(text) if s.strip()]
    if len(sents) < 2:
        return text
    kept = [s for s in sents if not _CLICHE_RE.search(s)]
    if len(kept) >= 2:
        return " ".join(kept)
    return text


def _dedupe_sentences(text: str, max_sents: int = 4) -> str:
    """Remove near-duplicate sentences to keep summaries dense and non-repetitive."""
    sents = [s.strip() for s in _SENT_SPLIT_RE.split(text) if s.strip()]
    seen: set[str] = set()
    out: List[str] = []
    for s in sents:
        key = re.sub(r"\s+", " ", s.lower()).strip()
        if key in seen:
            continue
        seen.add(key)
        out.append(s)
        if len(out) >= max_sents:
            break
    return " ".join(out)


def _fix_double_words(text: str) -> str:
    """Fix accidental repeated words or short repeated sequences."""
    return re.sub(r"\b(\w+)(\s+\1\b)+", r"\1", text, flags=re.IGNORECASE)


def _enforce_subject_lead(text: str, main_subject: str) -> str:
    """Ensure the first sentence begins with 'The {main_subject}' and reduce issuer ambiguity."""
    t = text.strip()
    if not t:
        return t

    if main_subject and main_subject.lower() != "the company":
        t = re.sub(r"\bthe company\b", f"the {main_subject}", t, flags=re.IGNORECASE)

    sents = [s.strip() for s in _SENT_SPLIT_RE.split(t) if s.strip()]
    if not sents:
        return t

    need_prefix = f"The {main_subject}"
    if sents[0].startswith(need_prefix):
        return " ".join(sents)

    if sents[0].startswith("The "):
        sents[0] = need_prefix + " " + re.sub(r"^The\s+\S+\s*", "", sents[0]).strip()
    else:
        sents[0] = need_prefix + " " + sents[0]

    return " ".join(sents)


def clean_model_output(text: str, main_subject: str = "") -> str:
    """Post-process generated summaries to remove boilerplate, clichés, and redundancy."""
    if not isinstance(text, str):
        return ""
    t = text.strip()
    t = re.sub(r"^(impact\s+summary\s*[:\-]\s*)", "", t, flags=re.IGNORECASE).strip()

    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
    lines = [ln for ln in lines if not _HEADING_LINE_RE.match(ln)]
    t = " ".join(lines)
    t = _INLINE_HEADING_RE.sub("", t)
    t = _WHITESPACE_RE.sub(" ", t).strip()

    t = _remove_cliches_safely(t)
    t = _fix_double_words(t)
    t = _dedupe_sentences(t, max_sents=4)
    if main_subject:
        t = _enforce_subject_lead(t, main_subject)

    if t and t[-1] not in ".!?":
        t = t.rstrip(" ,;:-") + "."

    return t


def extract_main_subject(text: str, fallback: str = "the company") -> str:
    """Extract an issuer/company name from the start of a disclosure using conservative heuristics."""
    if not isinstance(text, str):
        return fallback
    t = text.strip()
    if not t:
        return fallback

    head = t[:2500]
    for pat in _ISSUER_PATTERNS:
        m = pat.search(head)
        if m:
            cand = m.group(1).strip()
            cand = re.sub(r"\s{2,}", " ", cand).strip(" -–—:;,.")
            if len(cand) >= 3 and cand.lower() not in {"press release", "introduction", "table of contents"}:
                return cand
    return fallback


def parse_dates(df: pd.DataFrame, date_col: str) -> pd.DataFrame:
    """Parse the dataset date column using the expected '%d-%b-%y' format."""
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col], format="%d-%b-%y", errors="coerce")
    df = df.dropna(subset=[date_col]).reset_index(drop=True)
    return df


def filter_english_fasttext(df: pd.DataFrame, text_col: str, lid_model_path: str, threshold: float = 0.80) -> pd.DataFrame:
    """Filter rows to English text using a FastText language identification model."""
    model = fasttext.load_model(lid_model_path)
    texts = df[text_col].fillna("").astype(str).tolist()
    labels, probs = model.predict(texts, k=1)
    mask = [lbl[0] == "__label__en" and pr[0] >= threshold for lbl, pr in zip(labels, probs)]
    return df.loc[mask].reset_index(drop=True)


def prepare_dataframe(
    csv_path: str,
    lid_model_path: str,
    date_col: str = "Date",
    text_col: str = "Content",
    target_col: str = "Impact",
    lang_threshold: float = 0.80,
) -> pd.DataFrame:
    """Load, clean, and filter the dataset, and add a per-document issuer field."""
    df = pd.read_csv(csv_path)
    for col in [date_col, text_col, target_col]:
        if col not in df.columns:
            raise ValueError(f"Missing required column '{col}' in {csv_path}")

    df = parse_dates(df, date_col)

    df[text_col] = df[text_col].fillna("").astype(str).map(strip_boilerplate_lines)
    df[target_col] = df[target_col].fillna("").astype(str).map(clean_text)

    df = df[(df[text_col].str.len() > 60) & (df[target_col].str.len() > 10)].reset_index(drop=True)

    df = filter_english_fasttext(df, text_col=text_col, lid_model_path=lid_model_path, threshold=lang_threshold)
    df = df.sort_values(date_col).reset_index(drop=True)

    df["MainSubject"] = df[text_col].map(lambda x: extract_main_subject(x, fallback="the company"))
    return df


def chronological_train_test_split(df: pd.DataFrame, test_size: float = 0.20) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """Split the dataset by time order to avoid temporal leakage."""
    split_idx = int(len(df) * (1 - test_size))
    return df.iloc[:split_idx].reset_index(drop=True), df.iloc[split_idx:].reset_index(drop=True)


def chunk_tokenize_dataset(
    df: pd.DataFrame,
    tokenizer: AutoTokenizer,
    text_col: str,
    target_col: str,
    chunk_len: int,
    stride: int,
    max_target_len: int,
    keep_cols: List[str],
    *,
    add_impact_train_prefix: bool,
    min_chunk_ratio: float = 0.20,
) -> Dataset:
    """Tokenize the dataset with overlapping chunks and keep per-chunk metadata for downstream stages."""
    ds = Dataset.from_pandas(df, preserve_index=False)

    def preprocess(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]:
        mains = batch.get("MainSubject", ["the company"] * len(batch[text_col]))

        if add_impact_train_prefix:
            inputs = [
                IMPACT_TRAIN_PREFIX.format(main_subject=mains[i]) + batch[text_col][i]
                for i in range(len(batch[text_col]))
            ]
        else:
            inputs = batch[text_col]

        tok = tokenizer(
            inputs,
            max_length=chunk_len,
            truncation=True,
            stride=stride,
            return_overflowing_tokens=True,
            padding=False,
            return_attention_mask=True,
        )
        sample_map = tok.pop("overflow_to_sample_mapping")

        # Store decoded chunk text for the notes stage.
        tok["chunk_text"] = tokenizer.batch_decode(tok["input_ids"], skip_special_tokens=True)

        # Tokenize targets once per original document.
        tgt = tokenizer(
            batch[target_col],
            max_length=max_target_len,
            truncation=True,
            padding=False,
        )["input_ids"]

        labels: List[List[int]] = []
        doc_ids: List[int] = []
        chunk_ids: List[int] = []
        main_subjects: List[str] = []

        per_doc_counter: Dict[int, int] = {}
        for _, orig_pos in enumerate(sample_map):
            doc_id = int(indices[orig_pos])
            per_doc_counter.setdefault(doc_id, 0)
            c_id = per_doc_counter[doc_id]
            per_doc_counter[doc_id] += 1

            labels.append(tgt[orig_pos])
            doc_ids.append(doc_id)
            chunk_ids.append(c_id)
            main_subjects.append(mains[orig_pos] if isinstance(mains, list) else "the company")

        tok["labels"] = labels
        tok["doc_id"] = doc_ids
        tok["chunk_id"] = chunk_ids
        tok["MainSubject"] = main_subjects

        for c in keep_cols:
            tok[c] = [batch[c][orig_pos] for orig_pos in sample_map]

        return tok

    remove_cols = [c for c in ds.column_names if c not in keep_cols]
    out = ds.map(preprocess, batched=True, with_indices=True, remove_columns=remove_cols)

    # Drop tiny trailing chunks that tend to generate low-signal notes.
    min_tokens = max(8, int(min_chunk_ratio * chunk_len))

    def small_chunk_filter(ex: Dict) -> bool:
        am = ex.get("attention_mask", None)
        if am is None:
            return True
        n = int(sum(am)) if isinstance(am, list) else int(np.sum(am))
        return n >= min_tokens

    return out.filter(small_chunk_filter)


class WeightedSeq2SeqTrainer(Seq2SeqTrainer):
    """Trainer that reweights the loss for chunk_id==0 and strips metadata before generation."""

    def __init__(self, *args, first_chunk_weight: float = 1.35, **kwargs):
        super().__init__(*args, **kwargs)
        self.first_chunk_weight = float(first_chunk_weight)

    def compute_loss(self, model, inputs, return_outputs: bool = False, **kwargs):
        inputs = dict(inputs)

        labels = inputs.get("labels", None)
        chunk_id = inputs.pop("chunk_id", None)

        # These keys are never part of the model forward signature.
        inputs.pop("doc_id", None)
        inputs.pop("chunk_text", None)
        inputs.pop("MainSubject", None)

        outputs = model(**inputs)
        logits = outputs.get("logits", None)

        if labels is None or logits is None:
            loss = outputs["loss"] if "loss" in outputs else outputs[0]
            return (loss, outputs) if return_outputs else loss

        vocab_size = logits.size(-1)
        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction="none")

        loss_tok = loss_fct(logits.view(-1, vocab_size), labels.view(-1))
        loss_tok = loss_tok.view(labels.size(0), -1)

        mask = (labels != -100).float()
        denom = mask.sum(dim=1).clamp_min(1.0)
        loss_per_sample = (loss_tok * mask).sum(dim=1) / denom

        if chunk_id is not None:
            w = torch.ones_like(loss_per_sample)
            w = torch.where(chunk_id == 0, w * self.first_chunk_weight, w)
            loss = (loss_per_sample * w).mean()
        else:
            loss = loss_per_sample.mean()

        return (loss, outputs) if return_outputs else loss

    def prediction_step(self, model, inputs, prediction_loss_only: bool, ignore_keys=None):
        # During evaluation/prediction, Trainer may call generate(). Remove keys that would crash generate().
        inputs = dict(inputs)
        inputs.pop("chunk_id", None)
        inputs.pop("doc_id", None)
        inputs.pop("chunk_text", None)
        inputs.pop("MainSubject", None)
        return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)


def _estimate_total_train_steps(n_train_examples: int, train_bs: int, grad_accum: int, epochs: float) -> int:
    """Estimate optimizer update steps for warmup scheduling."""
    steps_per_epoch = math.ceil(n_train_examples / max(1, train_bs * grad_accum))
    return int(math.ceil(steps_per_epoch * epochs))


def build_trainer(
    model_name: str,
    output_dir: str,
    train_tok: Dataset,
    eval_tok: Dataset,
    max_target_len: int,
    lr: float,
    epochs: float,
    train_bs: int,
    eval_bs: int,
    grad_accum: int,
    eval_steps: int,
    save_steps: int,
    seed: int,
    fp16: bool,
    first_chunk_weight: float,
    warmup_ratio: float,
) -> Tuple[Seq2SeqTrainer, AutoTokenizer]:
    """Create the seq2seq trainer with stable scheduling and generation-based evaluation."""
    set_seed(seed)
    device = "cuda" if torch.cuda.is_available() else "cpu"

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

    collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding="longest",
        label_pad_token_id=-100,
    )

    total_steps = _estimate_total_train_steps(len(train_tok), train_bs, grad_accum, epochs)
    warmup_steps = max(1, int(warmup_ratio * total_steps))

    args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        logging_first_step=True,
        logging_steps=max(10, eval_steps // 10),
        learning_rate=lr,
        lr_scheduler_type="constant_with_warmup",
        warmup_steps=warmup_steps,
        num_train_epochs=epochs,
        per_device_train_batch_size=train_bs,
        per_device_eval_batch_size=eval_bs,
        gradient_accumulation_steps=grad_accum,
        eval_strategy="steps",
        eval_steps=eval_steps,
        save_steps=save_steps,
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="rougeL",
        greater_is_better=True,
        weight_decay=0.01,
        predict_with_generate=True,
        generation_max_length=max_target_len,
        fp16=bool(fp16 and torch.cuda.is_available()),
        bf16=False,
        remove_unused_columns=False,
        report_to="none",
    )

    scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)

    def compute_metrics(eval_pred) -> Dict[str, float]:
        preds, labels = eval_pred
        if isinstance(preds, (tuple, list)):
            preds = preds[0]

        preds = np.asarray(preds)
        labels = np.asarray(labels)

        pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
        vocab_size = getattr(tokenizer, "vocab_size", None)

        preds = preds.astype(np.int64, copy=False)
        preds = np.where(preds < 0, pad_id, preds)
        if vocab_size is not None:
            preds = np.where(preds >= vocab_size, pad_id, preds)

        labels = np.where(labels != -100, labels, pad_id)

        pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)
        label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)

        f1s = [scorer.score(r, p)["rougeL"].fmeasure for p, r in zip(pred_str, label_str)]
        return {"rougeL": float(np.mean(f1s))}

    trainer = WeightedSeq2SeqTrainer(
        model=model,
        args=args,
        train_dataset=train_tok,
        eval_dataset=eval_tok,
        data_collator=collator,
        compute_metrics=compute_metrics,
        first_chunk_weight=first_chunk_weight,
    )
    return trainer, tokenizer


def enforce_bullets(text: str, min_bullets: int = 4, max_bullets: int = 8) -> str:
    """Ensure the notes output is in bullet form, even when the model drifts."""
    if not isinstance(text, str):
        text = ""
    t = text.strip()
    t = _INLINE_HEADING_RE.sub("", t).strip()

    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
    bullets = [ln for ln in lines if ln.startswith("-")]
    if len(bullets) >= 2:
        return "\n".join(bullets[:max_bullets])

    t = re.sub(r"^(BULLETS:|NOTES:|SUMMARY:)\s*", "", t, flags=re.IGNORECASE).strip()
    sents = [s.strip(" -\t") for s in _SENT_SPLIT_RE.split(t) if s.strip()]
    if not sents:
        return "- (no extractable facts)"
    sents = sents[:max_bullets]
    return "\n".join([f"- {s}" for s in sents])


def generate_notes_per_chunk(
    notes_model,
    notes_tokenizer,
    chunk_texts: List[str],
    main_subjects: List[str],
    batch_size: int,
    device: str,
    max_new_tokens: int,
    num_beams: int,
) -> List[str]:
    """Generate factual bullet notes for each chunk."""
    prompts = [make_chunk_notes_prompt(t, ms) for t, ms in zip(chunk_texts, main_subjects)]
    preds: List[str] = []
    notes_model.eval()

    with torch.no_grad():
        for i in range(0, len(prompts), batch_size):
            enc = notes_tokenizer(
                prompts[i : i + batch_size],
                max_length=1024,
                truncation=True,
                padding=True,
                return_tensors="pt",
            ).to(device)

            out = notes_model.generate(
                **enc,
                max_new_tokens=max_new_tokens,
                min_new_tokens=max(40, int(0.35 * max_new_tokens)),
                num_beams=num_beams,
                early_stopping=True,
                no_repeat_ngram_size=3,
                repetition_penalty=1.06,
                length_penalty=0.85,
            )
            preds.extend(notes_tokenizer.batch_decode(out, skip_special_tokens=True))

    return [enforce_bullets(p) for p in preds]


def _reduce_notes_recursively(
    impact_model,
    impact_tokenizer,
    notes: List[str],
    main_subject: str,
    device: str,
    max_new_tokens: int,
    num_beams: int,
    group_size: int = 10,
) -> List[str]:
    """Recursively consolidate long note lists to stay within the model's context window."""
    cleaned = [n.strip() for n in notes if isinstance(n, str) and n.strip()]
    if len(cleaned) <= group_size:
        return cleaned

    reduced: List[str] = []
    impact_model.eval()

    with torch.no_grad():
        for i in range(0, len(cleaned), group_size):
            group = cleaned[i : i + group_size]
            prompt = (
                ANALYST_ROLE
                + f"Main Subject (issuer): {main_subject}\n"
                + "Task: Consolidate the following bullet notes into 6–10 bullet points.\n"
                + "Rules (STRICT):\n"
                + "- Preserve entity attribution (who did what) and counterparties.\n"
                + "- Preserve ALL numbers/units/currencies/durations exactly.\n"
                + "- Preserve technical financial terms verbatim.\n"
                + "- NO filler, NO opinions, NO new facts.\n"
                + "- Output ONLY bullets starting with '-'.\n\n"
                + "NOTES:\n"
                + "\n".join(group)
                + "\n\nBULLETS:\n"
            )
            enc = impact_tokenizer(
                [prompt],
                max_length=1024,
                truncation=True,
                padding=True,
                return_tensors="pt",
            ).to(device)

            out = impact_model.generate(
                **enc,
                max_new_tokens=max(110, int(0.55 * max_new_tokens)),
                min_new_tokens=60,
                num_beams=max(4, num_beams),
                early_stopping=True,
                no_repeat_ngram_size=3,
                repetition_penalty=1.05,
                length_penalty=0.90,
            )
            txt = impact_tokenizer.batch_decode(out, skip_special_tokens=True)[0]
            reduced.append(enforce_bullets(txt, max_bullets=12))

    if len(reduced) > group_size:
        return _reduce_notes_recursively(
            impact_model,
            impact_tokenizer,
            reduced,
            main_subject,
            device,
            max_new_tokens,
            num_beams,
            group_size,
        )

    return reduced


def generate_impact_summaries(
    impact_model,
    impact_tokenizer,
    chunk_meta: pd.DataFrame,
    batch_size: int,
    device: str,
    max_new_tokens: int,
    num_beams: int,
    reduce_group_size: int,
    length_penalty: float,
) -> pd.DataFrame:
    """Generate a document-level impact summary for each doc_id using recursive note reduction."""
    chunk_meta = chunk_meta.sort_values(["doc_id", "chunk_id"]).reset_index(drop=True)

    doc_text = chunk_meta.groupby("doc_id", sort=False)["Content"].first().to_dict()
    doc_subject = chunk_meta.groupby("doc_id", sort=False)["MainSubject"].first().to_dict()

    doc_ids: List[int] = []
    prompts: List[str] = []

    for doc_id, g in chunk_meta.groupby("doc_id", sort=False):
        doc_id_int = int(doc_id)
        main_subject = str(doc_subject.get(doc_id_int, "the company"))

        final_notes_blocks = _reduce_notes_recursively(
            impact_model=impact_model,
            impact_tokenizer=impact_tokenizer,
            notes=g["chunk_pred"].tolist(),
            main_subject=main_subject,
            device=device,
            max_new_tokens=max_new_tokens,
            num_beams=num_beams,
            group_size=reduce_group_size,
        )

        doc_ids.append(doc_id_int)
        prompts.append(make_reduce_prompt(final_notes_blocks, main_subject=main_subject))

    preds: List[str] = []
    impact_model.eval()

    with torch.no_grad():
        for i in range(0, len(prompts), batch_size):
            enc = impact_tokenizer(
                prompts[i : i + batch_size],
                max_length=1024,
                truncation=True,
                padding=True,
                return_tensors="pt",
            ).to(device)

            out = impact_model.generate(
                **enc,
                max_new_tokens=max_new_tokens,
                min_new_tokens=max(70, int(0.45 * max_new_tokens)),
                num_beams=num_beams,
                early_stopping=True,
                no_repeat_ngram_size=4,
                repetition_penalty=1.06,
                length_penalty=length_penalty,
            )
            preds.extend(impact_tokenizer.batch_decode(out, skip_special_tokens=True))

    cleaned: List[str] = []
    for did, p in zip(doc_ids, preds):
        main_subject = str(doc_subject.get(int(did), "the company"))
        cleaned.append(clean_model_output(p, main_subject=main_subject))

    return pd.DataFrame({"doc_id": doc_ids, "text": [doc_text[i] for i in doc_ids], "pred_summary": cleaned})


def add_rouge_scores(pred_df: pd.DataFrame, gold_summaries: List[str]) -> Tuple[pd.DataFrame, Dict[str, float]]:
    """Compute ROUGE-L F1 per document and return the mean score."""
    scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)
    scores: List[float] = []

    for _, row in pred_df.iterrows():
        doc_id = int(row["doc_id"])
        pred = row["pred_summary"] or ""
        gold = gold_summaries[doc_id] if 0 <= doc_id < len(gold_summaries) else ""
        scores.append(float(scorer.score(gold, pred)["rougeL"].fmeasure))

    out = pred_df.copy()
    out["rougeL_f1"] = scores
    return out, {"rougeL_f1_mean": float(np.mean(scores)) if scores else 0.0}


def main() -> None:
    """Run end-to-end training and evaluation for the impact summarizer."""
    ap = argparse.ArgumentParser()

    ap.add_argument("--data", default="dataset.csv")
    ap.add_argument("--lid", default="lid.176.bin")

    ap.add_argument("--impact_model_name", default="google/flan-t5-large")
    ap.add_argument("--notes_model_name", default="google/flan-t5-large")

    ap.add_argument("--output_dir", default="./t5_impact_large")
    ap.add_argument("--pred_csv", default="pred.csv")
    ap.add_argument("--test_size", type=float, default=0.20)

    ap.add_argument("--chunk_len", type=int, default=768)
    ap.add_argument("--stride", type=int, default=384)

    ap.add_argument("--max_impact_new_tokens", type=int, default=240)
    ap.add_argument("--max_notes_new_tokens", type=int, default=180)

    ap.add_argument("--lr", type=float, default=8e-5)
    ap.add_argument("--warmup_ratio", type=float, default=0.10)
    ap.add_argument("--epochs", type=float, default=6.0)
    ap.add_argument("--train_bs", type=int, default=2)
    ap.add_argument("--eval_bs", type=int, default=2)
    ap.add_argument("--grad_accum", type=int, default=8)
    ap.add_argument("--eval_steps", type=int, default=250)
    ap.add_argument("--save_steps", type=int, default=250)
    ap.add_argument("--seed", type=int, default=42)

    ap.add_argument("--first_chunk_weight", type=float, default=1.35)

    ap.add_argument("--gen_bs", type=int, default=4)
    ap.add_argument("--num_beams", type=int, default=6)
    ap.add_argument("--length_penalty", type=float, default=1.20)

    ap.add_argument("--reduce_group_size", type=int, default=10)
    ap.add_argument("--min_chunk_ratio", type=float, default=0.20)

    ap.add_argument("--fp16", action="store_true")

    args = ap.parse_args()

    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"[Init] Device = {device}", flush=True)
    print(f"[Init] Data = {args.data} | LID = {args.lid}", flush=True)
    print(f"[Init] Impact model = {args.impact_model_name} | Notes model = {args.notes_model_name}", flush=True)
    print(f"[Init] Output dir = {args.output_dir} | pred.csv = {args.pred_csv}", flush=True)

    print("\n[1/7] Loading + cleaning dataset...", flush=True)
    df = prepare_dataframe(args.data, args.lid, date_col="Date", text_col="Content", target_col="Impact")
    print(f"[1/7] Rows after prep: {len(df)}", flush=True)

    print("\n[2/7] Chronological train/test split...", flush=True)
    train_df, test_df = chronological_train_test_split(df, test_size=args.test_size)
    print(f"[2/7] Train rows: {len(train_df)} | Test rows: {len(test_df)}", flush=True)

    print("\n[3/7] Building tokenizer and chunking datasets...", flush=True)
    impact_tokenizer_base = AutoTokenizer.from_pretrained(args.impact_model_name)

    print("[3/7] Chunking TRAIN set...", flush=True)
    train_tok_full = chunk_tokenize_dataset(
        train_df,
        tokenizer=impact_tokenizer_base,
        text_col="Content",
        target_col="Impact",
        chunk_len=args.chunk_len,
        stride=args.stride,
        max_target_len=min(256, args.max_impact_new_tokens),
        keep_cols=["Date", "Content", "MainSubject"],
        add_impact_train_prefix=True,
        min_chunk_ratio=args.min_chunk_ratio,
    )
    print(f"[3/7] Train chunks: {len(train_tok_full)}", flush=True)

    print("[3/7] Chunking TEST set...", flush=True)
    test_tok_full = chunk_tokenize_dataset(
        test_df.reset_index(drop=True),
        tokenizer=impact_tokenizer_base,
        text_col="Content",
        target_col="Impact",
        chunk_len=args.chunk_len,
        stride=args.stride,
        max_target_len=min(256, args.max_impact_new_tokens),
        keep_cols=["Date", "Content", "MainSubject"],
        add_impact_train_prefix=False,
        min_chunk_ratio=args.min_chunk_ratio,
    )
    print(f"[3/7] Test chunks: {len(test_tok_full)}", flush=True)

    print("\n[4/7] Preparing Trainer datasets (dropping non-numeric columns)...", flush=True)
    train_tok = train_tok_full.remove_columns(["Date", "Content", "doc_id", "chunk_text", "MainSubject"])
    eval_tok = test_tok_full.remove_columns(["Date", "Content", "doc_id", "chunk_text", "MainSubject"])
    print(f"[4/7] Trainer train size: {len(train_tok)} | eval size: {len(eval_tok)}", flush=True)

    print("\n[5/7] Building Trainer...", flush=True)
    os.makedirs(args.output_dir, exist_ok=True)
    trainer, _ = build_trainer(
        model_name=args.impact_model_name,
        output_dir=args.output_dir,
        train_tok=train_tok,
        eval_tok=eval_tok.select(range(min(2000, len(eval_tok)))) if len(eval_tok) else eval_tok,
        max_target_len=min(256, args.max_impact_new_tokens),
        lr=args.lr,
        epochs=args.epochs,
        train_bs=args.train_bs,
        eval_bs=args.eval_bs,
        grad_accum=args.grad_accum,
        eval_steps=args.eval_steps,
        save_steps=args.save_steps,
        seed=args.seed,
        fp16=args.fp16,
        first_chunk_weight=args.first_chunk_weight,
        warmup_ratio=args.warmup_ratio,
    )

    print("[5/7] Starting training...", flush=True)
    trainer.train()

    print("[5/7] Saving trained model + tokenizer...", flush=True)
    trainer.save_model(args.output_dir)
    impact_tokenizer_base.save_pretrained(args.output_dir)

    print("\n[6/7] Loading models for inference (notes + trained impact)...", flush=True)
    notes_tokenizer = AutoTokenizer.from_pretrained(args.notes_model_name)
    notes_model = AutoModelForSeq2SeqLM.from_pretrained(args.notes_model_name).to(device)

    impact_tokenizer = AutoTokenizer.from_pretrained(args.output_dir)
    impact_model = AutoModelForSeq2SeqLM.from_pretrained(args.output_dir).to(device)

    print("[6/7] Building chunk metadata for test set...", flush=True)
    meta = (
        test_tok_full.remove_columns(["Date"])
        .select_columns(["doc_id", "chunk_id", "Content", "chunk_text", "MainSubject"])
        .to_pandas()
    )
    print(f"[6/7] Meta rows (test chunks): {len(meta)}", flush=True)

    print("[6/7] Generating NOTES per chunk...", flush=True)
    chunk_notes = generate_notes_per_chunk(
        notes_model=notes_model,
        notes_tokenizer=notes_tokenizer,
        chunk_texts=meta["chunk_text"].fillna("").astype(str).tolist(),
        main_subjects=meta["MainSubject"].fillna("the company").astype(str).tolist(),
        batch_size=args.gen_bs if device == "cuda" else max(1, args.gen_bs // 2),
        device=device,
        max_new_tokens=args.max_notes_new_tokens,
        num_beams=max(4, args.num_beams),
    )
    meta["chunk_pred"] = chunk_notes
    print("[6/7] NOTES generation done.", flush=True)

    print("[6/7] Reducing NOTES to document-level impact summaries...", flush=True)
    doc_pred = generate_impact_summaries(
        impact_model=impact_model,
        impact_tokenizer=impact_tokenizer,
        chunk_meta=meta,
        batch_size=args.gen_bs if device == "cuda" else max(1, args.gen_bs // 2),
        device=device,
        max_new_tokens=args.max_impact_new_tokens,
        num_beams=args.num_beams,
        reduce_group_size=args.reduce_group_size,
        length_penalty=args.length_penalty,
    )
    print(f"[6/7] Generated doc summaries: {len(doc_pred)}", flush=True)

    print("\n[7/7] Computing ROUGE-L vs gold Impact labels...", flush=True)
    gold = test_df.reset_index(drop=True)["Impact"].tolist()
    doc_pred_scored, overall = add_rouge_scores(doc_pred, gold)
    print(f"[7/7] Overall ROUGE-L F1 mean: {overall['rougeL_f1_mean']:.4f}", flush=True)

    print("[7/7] Writing pred.csv...", flush=True)
    out = doc_pred_scored[["text", "pred_summary", "rougeL_f1"]].copy()
    footer = pd.DataFrame([{"text": "__OVERALL__", "pred_summary": "", "rougeL_f1": overall["rougeL_f1_mean"]}])
    pd.concat([out, footer], ignore_index=True).to_csv(args.pred_csv, index=False)
    print(f"[Done] Wrote: {args.pred_csv}", flush=True)


if __name__ == "__main__":
    main()
